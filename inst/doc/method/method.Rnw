% !TEX TS-program = knitr
\documentclass{article}
 
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{color}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{listings}
\usepackage{mathrsfs}
\usepackage{natbib}
\usepackage[nottoc]{tocbibind}
\usepackage{url}

\providecommand{\all}{\ \forall \ }
\providecommand{\bs}{\backslash}
\providecommand{\e}{\varepsilon}
\providecommand{\E}{\ \exists \ }
\providecommand{\lm}[2]{\lim_{#1 \rightarrow #2}}
\providecommand{\m}[1]{\mathbb{#1}}
\providecommand{\nv}{{}^{-1}}
\providecommand{\ov}[1]{\overline{#1}}
\providecommand{\p}{\newpage}
\providecommand{\q}{$\quad$ \newline}
\providecommand{\rt}{\rightarrow}
\providecommand{\Rt}{\Rightarrow}
\providecommand{\vc}[1]{\boldsymbol{#1}}
\providecommand{\wh}[1]{\widehat{#1}}

%\renewcommand\bibname{References}
%\renewcommand{\thesection}{Problem \arabic{section}}C
%\renewcommand{\thesubsection}{Part \alph{subsection}}

\fancyhead{}
\fancyfoot{}
\fancyhead[R]{\thepage}
\fancyhead[C]{Landau}

\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=blue
}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{ 
  language=C,                % the language of the code
  basicstyle=\Large,           % the size of the fonts that are used for the code
  numberstyle= \tiny \color{white},  % the style that is used for the line-numbers
  stepnumber=2,                   % the step between two line-numbers. 
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},      % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=lrb,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text 
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=t,                   % sets the caption-position 
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  title=\lstname,                   % show the filename of files included with \lstinputlisting;
  keywordstyle=\color{blue},          % keyword style
  commentstyle=\color{gray},       % comment style
  stringstyle=\color{dkgreen},         % string literal style
  escapeinside={\%*}{*)},            % if you want to add LaTeX within your code
  morekeywords={*, ...},               % if you want to add more keywords to the set
  xleftmargin=0.053in, % left horizontal offset of caption box
  xrightmargin=-.03in % right horizontal offset of caption box
}

\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\parbox{\textwidth}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}\vskip-0.05in}}
\captionsetup[lstlisting]{format = listing, labelfont = white, textfont = white}
% For caption-free listings, comment out the 3 lines above and uncomment the 2 lines below.
% \captionsetup{labelformat = empty, labelsep = none}
% \lstset{frame = single}

<<echo = F>>=
options(width = 60) # R output width
@

\begin{document}
\begin{titlepage}
\begin{center}

\vspace*{4cm}
\hrule 
\vspace{0.4cm}
{ \huge \bfseries A Fully Bayesian Model for Gene Expression Heterosis in RNA-seq Data}
\vspace{0.4cm}
\hrule 

\vspace{1cm}
\Large
\begin{center}
Will Landau \\ $\quad$ \\
Department of Statistics \\
Iowa State University \\ $\quad$ \\
\today
\end{center}

\vfill
\large
\end{center}
\end{titlepage}

\newpage 
\pagestyle{fancy}
\setcounter{page}{1}
\pagenumbering{roman}
\tableofcontents 

\newpage
\setcounter{page}{1}
\pagenumbering{arabic}
%\fancyhead[C]{\thesection}

\begin{flushleft}

\section{Introduction} \label{sec:intro}

This writeup explains a fully Bayesian Markov chain Monte Carlo method for modeling RNA-seq data. The hierarchical model featured focuses on heterosis, or hybrid vigor, a phenomenon that concerns two parental genetic lines and an hybrid line. For each gene in an RNA-seq dataset, we consider three types of heterosis at the level of gene expression:

\begin{enumerate}
\item High parent heterosis: the gene is significantly more expressed in the hybrid than in either of the parent lines.
\item Low parent heterosis: the gene is significantly less expressed in the hybrid than in either of the parent lines.
\item Mid parent heterosis: the expression level of the gene in the hybrid is significantly different from the average of the parental expression levels.
\end{enumerate}

Let $y_{g,n}$ be the expression level of gene $g$  ($g = 1, \ldots, G$) in sample $n$ ($n = 1, \ldots, N$). The samples come from one of three groups: group 1, the first parent, group 2, the hybrid, and group 3, the second parent. Hence, we define:

\begin{itemize}
\item $\mu_{g1}$: mean expression level of gene $g$ in the first parent
\item $\mu_{g2}$: mean expression level of gene $g$ in the hybrid
\item $\mu_{g3}$: mean expression level of gene $g$ in the second parent
\end{itemize}

In the model below, there are three quantities of primary interest:

\begin{itemize}
\item $\phi_g = \frac{\mu_{g1} + \mu_{g3}}{2}$, the parental mean expression level of gene $g$.
\item $\alpha_g = \frac{\mu_{g1} - \mu_{g3}}{2}$, half the parental difference in expression levels of gene $g$.
\item $\delta_g = \mu_{g2} - \phi_g$, the overexpression of gene $g$ in the hybrid relative to the parental mean.
\end{itemize}

With MCMC samples of these quantities, for some threshold $\e > 0$, we can calculate empirical estimates of the following probabilities of interest:

\begin{itemize}
\item $P(|\alpha_g| \ge \e \mid \vc{y})$, the probability of differential expression.
\item $P(\delta_g > |\alpha_g| \ \mid \ \vc{y}) $, the probability of high parent heterosis.
\item $P(\delta_g < -|\alpha_g| \ \mid \ \vc{y})$, the probability of low parent heterosis.
\item $P(|\delta_g| \ge \e \mid \vc{y})$, the probability of mid parent heterosis.
\end{itemize}

\section{The Model} \label{sec:model}

\begin{align*}
&y_{g,n} \stackrel{\text{ind}}{\sim} \text{Poisson}(\exp(\rho_n + \e_{g, n} + \eta(g, n))) \\
&\qquad \rho_n \stackrel{\text{ind}}{\sim} \text{N}(0, \sigma_\rho^2) \\
& \qquad \qquad \sigma_\rho \stackrel{\text{}}{\sim} \text{U}(0, s_\rho) \\
& \qquad \e_{g, n} \stackrel{\text{ind}}{\sim} \text{N}(0, \gamma_g^2) \\
& \qquad \qquad \gamma_g^2 \stackrel{\text{ind}}{\sim} \text{Inv-Gamma}\left (\text{shape} = \frac{\nu}{2}, \ \text{scale} =  \frac{\nu \tau^2}{2} \right) \\
& \qquad \qquad \qquad \nu \stackrel{\text{}}{\sim} \text{U}(0, d) \\
& \qquad \qquad \qquad \tau^2 \stackrel{\text{}}{\sim} \text{Gamma}(\text{shape} = a, \text{rate} = b) \\
& \qquad \phi_g \stackrel{\text{ind}}{\sim} \text{N}(\theta_\phi, \sigma_\phi^2) \\
& \qquad \qquad \theta_\phi \stackrel{\text{}}{\sim} \text{N}(0, c_{\phi }^2) \\
& \qquad \qquad \sigma_\phi \stackrel{\text{}}{\sim} \text{U}(0, s_{\phi}) \\
& \qquad \alpha_g \stackrel{\text{ind}}{\sim} \text{N}( \theta_\alpha, \sigma_\alpha^2) \\
& \qquad \qquad \theta_\alpha \stackrel{\text{}}{\sim} \text{N}(0, c_{\alpha}^2) \\
& \qquad \qquad \sigma_\alpha \stackrel{\text{}}{\sim} \text{U}(0, s_{\alpha}) \\
& \qquad \delta_g \stackrel{\text{ind}}{\sim} \text{N}(\theta_\delta, \sigma_{\delta}^2) \\
& \qquad \qquad \theta_\delta \stackrel{\text{}}{\sim} \text{N}(0, c_{\delta}^2) \\
& \qquad \qquad \sigma_\delta \stackrel{\text{}}{\sim} \text{U}(0, s_{\delta}) \\
\end{align*}

where:
\begin{itemize}
\item Conditional independence is implied unless otherwise specified.
\item The parameters to the left of the ``$\sim$" are implicitly conditioned on the parameters to the right.
\item  $\eta(g, n)$ is the function given by:
\begin{align*}
\eta(g, n) = \begin{cases}
\phi_g - \alpha_g & \text{ library $n$ is in treatment group 1 (parent 1)} \\
\phi_g + \delta_g & \text{ library $n$ is in treatment group 2 (hybrid)} \\
\phi_g + \alpha_g & \text{ library $n$ is in treatment group 3 (parent 2)} \\
\end{cases}
\end{align*}

\end{itemize}






\section{The Full Conditional Distributions}

Define:

\begin{itemize}
\item  $k(n)$ = treatment group of library $n$.
\item $\lambda_{g, n} = \exp(\rho_n + \e_{g, n} + \eta(g, n))$ )
\item $G_\alpha = $ number of genes for which $\alpha_g \ne 0$
\item $G_\delta = $ number of genes for which $\delta_g \ne 0$
\end{itemize}

Then: 

\begin{align*}
p(\nu \mid \cdots) &\propto  \exp \left ( - \log \Gamma(\nu/2) + \frac{ \nu}{2} \log \left ( \frac{\nu \tau^2}{2} \right ) -  \nu \frac{1}{G} \sum_{g = 1}^G \left [ \log \gamma_g + \frac{\tau^2}{2} \frac{1}{\gamma_g^2} \right ] \right )^G \\
& \qquad \times I(0 < \nu < d) \\
p(\rho_n \mid \cdots) &\propto\exp \left (\rho_n G\ov{y}_{.n} - \frac{\rho_n^2}{2 \sigma_\rho^2} -\exp(\rho_n) \sum_{g = 1}^G \exp( \e_{g, n} + \eta(g, n)) \right ) \\
p(\e_{g, n} \mid \cdots) &\propto \exp \left (y_{g, n} \e_{g, n} - \frac{\e_{g, n}^2}{2 \gamma_g^2}  - \exp( \e_{g, n} ) \exp(\rho_n + \eta(g, n))  \right) \\
p(\phi_g \mid \cdots) &\propto \exp \left ( \phi_g N \ov{y}_{g.}  - \frac{(\phi_g - \theta_\phi)^2}{2 \sigma_\phi^2} - \exp(\phi_g) \left [\exp(-\alpha_g) \sum_{k(n) = 1}  \exp (\rho_n + \e_{g, n})   \right . \right . \\
& \left . \qquad +\exp(\delta_g) \sum_{k(n) = 2}  \exp (\rho_n + \e_{g, n} )   + \left . \exp(\alpha_g) \sum_{k(n) = 3}  \exp (\rho_n + \e_{g, n} )  \right ] \right ) \\
p(\alpha_g \mid \cdots) &\propto \exp \left (\alpha_g \left (\sum_{k(n) = 3} y_{g, n} - \sum_{k(n) = 1} y_{g, n} \right ) - \frac{(\alpha_g - \theta_\alpha)^2}{2 \sigma_\alpha^2} \right . \\
& \left .\qquad - \exp(\alpha_g) \exp(\phi_g) \sum_{k(n) = 3} \exp (\rho_n + \e_{g, n}) \right . \\
& \qquad \left .- \exp(-\alpha_g) \exp(\phi_g) \sum_{k(n) = 1}  \exp (\rho_n + \e_{g, n}) \right ) \\
p(\delta_g \mid \cdots ) & \propto \exp \left (\delta_g \sum_{k(n) = 2} y_{g, n}  - \frac{(\delta_g - \theta_\delta)^2}{2 \sigma_\delta^2}  - \exp(\delta_g) \exp(\phi_g) \sum_{k(n) \ne 2} \exp (\rho_n + \e_{g, n})  \right ) 
\end{align*}
\begin{align*}
p(\theta_\phi \mid \cdots) &= \text{N} \left ( \theta_\phi \ \left | \ \frac{ c_\phi^2 \sum_{g = 1}^G \phi_g}{G c_\phi^2 + \sigma_\phi^2}, \ \frac{c_\phi^2 \sigma_\phi^2}{Gc_\phi^2 + \sigma_\phi^2} \right . \right ) \\
p(\theta_\alpha \mid \cdots) &= N \left ( \theta_\alpha \ \left | \ \frac{c_\alpha^2 \sum_{g = 1}^G \alpha_g}{G_\alpha c_\alpha^2 + \sigma_\alpha^2}, \right . \ \frac{c_\alpha^2 \sigma_\alpha^2}{G_\alpha c_\alpha^2 + \sigma_\alpha^2} \right ) \\
p(\theta_\delta \mid \cdots) &= N \left ( \theta_\delta \ \left | \ \frac{c_\delta^2 \sum_{g = 1}^G \delta_g}{G_\delta c_\delta^2 + \sigma_\delta^2}, \right . \ \frac{c_\delta^2 \sigma_\delta^2}{G_\delta c_\delta^2 + \sigma_\delta^2} \right ) \\
p(\tau^2 \mid \cdots) &= \text{Gamma} \left ( \tau^2 \mid \text{shape} =  a + \frac{G\nu}{2}, \ \text{rate} =  b + \frac{\nu}{2} \sum_{g = 1}^G \frac{1}{\gamma_g^2} \right ) \\
p \left ( \left . \frac{1}{\gamma_g^2} \ \right | \ \cdots \right ) &= \text{Gamma} \left ( \left . \frac{1}{\gamma_g^2} \ \right | \ \text{shape} = \frac{N + \nu}{2}, \  \text{rate} = \frac{1}{2} \left (\nu \tau^2 + \sum_{n  =1}^N \e_{g, n}^2 \right ) \right ) \\
p \left ( \frac{1}{\sigma_\rho^2} \mid \cdots \right) &= \text{Gamma} \left ( \frac{1}{\sigma_\rho^2} \mid \text{shape} = \frac{N - 1}{2}, \ \text{rate} =\frac{1}{2} {\sum_{n = 1}^N \rho_n^2} \right )  I \left (\frac{1}{\sigma_\rho^2} > \frac{1}{s_\rho^2} \right ) \\
p \left ( \left . \frac{1}{\sigma_\phi^2} \  \right | \  \cdots \right ) &= \text{Gamma} \left ( \text{shape} = \frac{G - 1}{2}, \ \text{rate} =  \frac{1}{2} \sum_{g = 1}^G (\phi_g - \theta_\phi)^2  \right )   \text{I} \left (\frac{1}{\sigma_\phi^2} >\frac{1}{ s_\phi^2} \right )\\
p \left ( \left . \frac{1}{\sigma_\alpha^2} \  \right | \  \cdots \right ) &= \text{Gamma} \left ( \text{shape} = \frac{G - 1}{2}, \ \text{rate} =  \frac{1}{2} \sum_{g = 1}^G (\alpha_g - \theta_\alpha)^2  \right )   \text{I} \left (\frac{1}{\sigma_\alpha^2} >\frac{1}{ s_\alpha^2} \right )\\
p \left ( \left . \frac{1}{\sigma_\delta^2} \  \right | \  \cdots \right ) &= \text{Gamma} \left ( \text{shape} = \frac{G - 1}{2}, \ \text{rate} =  \frac{1}{2} \sum_{g = 1}^G (\delta_g - \theta_\delta)^2  \right )   \text{I} \left (\frac{1}{\sigma_\delta^2} >\frac{1}{ s_\delta^2} \right )\\
\end{align*}

\section{A Metropolis-Hastings Algorithm for Sampling $\nu$}

\paragraph{} The full conditional distribution of $\nu$ is of the form,

\begin{align*}
p(\nu \mid \cdots) &\propto  \exp \left ( - \log \Gamma(\nu/2) + \frac{ \nu}{2} \log \left ( \frac{\nu \tau^2}{2} \right ) -  \nu \frac{1}{G} \sum_{g = 1}^G \left [ \log \gamma_g + \frac{\tau^2}{2} \frac{1}{\gamma_g^2} \right ] \right )^G \\
& \qquad \times I(0 < \nu < d) \\
&= \exp(h(\nu))
\intertext{where}
h(\nu) &= G \cdot \left  (- \log \Gamma(\nu/2) + \frac{ \nu}{2} \log \left ( \frac{\nu \tau^2}{2} \right ) -  \nu \frac{1}{G} \sum_{g = 1}^G \left [ \log \gamma_g + \frac{\tau^2}{2} \frac{1}{\gamma_g^2} \right ] \right ) 
\end{align*}

% Now, $\exp(G \nv h(\nu))$ can be well approximated by a gamma distribution. For some positive integer $X$, if I take 

Using GPUs to accelerate computation, $\exp(G \nv h(\nu))$ can be easily approximated (up to a proportionality constant) by a step function with thick tails, such as the one in the example below.

\begin{center}
\includegraphics[scale=.25]{step.png}
\end{center} 

%\begin{align*}
%M &= \frac{\sum_{x = 0}^X x \cdot \exp(G \nv h(\nu))}{\sum_{x = 0}^X \exp(G \nv h(\nu))} \\
%V &= \frac{\sum_{x = 0}^X (x - M)^2 \cdot \exp(G \nv h(\nu))}{\sum_{x = 0}^X \exp(G \nv h(\nu))} \\
%\end{align*}

%then,

%\begin{align*}
%\exp(G \nv h(\nu)) \mathop{\propto}^\cdot \text{Gamma}(\text{shape} = M^2/V, \text{rate} = M/V)
%\end{align*}

%As a demonstration, I simulate 10000 $\gamma_g$'s with $\nu = \tau = 1$. Visually, the agreement between $\exp(G \nv h(\nu))$ and its gamma approximation is excellent.

<<echo = F, eval = F>>=
d = 100
tau = 1
nu = 1
G = 10000
gam = 1/sqrt(rgamma(G, shape = nu/2, rate = nu * tau^2/2))

lp = function(nu){
  - lgamma(nu/2) + nu/2 * log(nu*tau^2/2) - nu* mean(log(gam) + tau^2/2 * 1/gam^2)
}

p = function(nu){
  exp(lp(nu))
}

x = 1:10000/1000
m = sum(x * p(x))/sum(p(x))
v = sum((x-m)^2 * p(x))/sum(p(x))

shape = m^2/v
rate = m/v

f = function(x){dgamma(x, shape = shape, rate = rate) /2}
curve(f, 0, 10, col = "blue")
xs = 1:10000/1000
ys = p(xs)/max(p(xs)) * .15
lines(x =xs, y = ys)
legend("topright", fill = c("black", "blue"), legend = c("Const * exp((1/G) * h(nu))", "Gamma approximation"))
@

%The upshot of all this is

%\begin{align*}
%p(\nu \mid \cdots) &\propto \exp(h(\nu)) \\
%&= \exp(G \nv h(\nu))^G\\
%&\approx [\text{Gamma}(M^2/V, M/V)]^G\\
%&\propto \left [\nu^{M^2/V - 1} \exp(M/V \cdot \nu) \right ]^G\\
%&= \nu^{G(M^2/V - 1)} \exp(G M/V \cdot \nu) \\
%&= \nu^{(GM^2/V - G + 1) - 1} \exp(G M/V \cdot \nu) \\
%\end{align*}

%Hence,

%\begin{align*}
%p(\nu \mid \cdots) \mathop{\propto}^\cdot \text{Gamma} \left ( \text{shape} = \frac{G M^2}{V} - G + 1, \ \text{rate} = \frac{G M}%{V} \right )
%\end{align*}

%Letting $q(\nu)$ be the log density of $\text{Gamma} \left ( \text{shape} = \frac{G M^2}{V} - G + 1, \ \text{rate} = \frac{G M}{V} \right )$, we have a Metropolis-Hastings algorithm for sampling $\nu$.

In practice, the step function will have a finer partition. Letting $q(\nu)$ be the log of the step function, we have a Metropolis-Hastings algorithm for sampling $\nu$.

\begin{enumerate}
\item Sample a proposal $\nu^*$ from a $q(\nu)$.
\item The acceptance probability is

\begin{align*}
p &= \min \left \{1, \ \frac{\exp(h(\nu^*))}{\exp(h(\nu^{(i)}))} \frac{\exp(q(\nu^{(i)}))}{\exp(q(\nu^{*}))} \right \}
\end{align*}

In practice, calculate $p$ on a log scale.

\begin{align*}
\log p &= \min \left \{0, \ h(\nu^*) - h(\nu^{(i)}) +  q(\nu^{(i)}) - q(\nu^{*}) \right \}
\end{align*}

\item Sample $u \sim U(0, 1)$

\item If $\log u < \log p$, set $\nu^{(i + 1)} = \nu^{*}$ (accept $\nu^*$).. Otherwise, set $\nu^{(i+1)} = \nu^{(i)}$ (do not except $\nu^*$).

\end{enumerate}












\section{A Metropolis-Hastings Algorithm for Sampling $\rho_n$, $\phi_g$, $\alpha_g$, $\delta_g$, and $\e_{g, n}$}


\paragraph{} $\rho_n$, $\phi_g$, $\alpha_g$, $\delta_g$, and $\e_{g, n}$ all have full conditional distributions of the form,

\begin{align*}
p(\theta \mid \cdots) \propto \exp \left (A \theta - B(\theta - C)^2 - D e^{\theta} - Ee^{-\theta} \right )
\end{align*}

where $\theta$ is the parameter of interest, $A, B, C, D$, and $E$ are constants, and $B, D, E \ge 0$. Note that $E$ is guaranteed to be 0 except when $\theta = \alpha_g$ for some $h$. Let $h(\theta)$ be the log kernel of $p(\theta \mid \cdots)$. Then,

\begin{align*}
h(\theta) = A \theta - B(\theta - C)^2 - D e^{\theta} - Ee^{-\theta}
\end{align*}

In addition, let $\wh{\theta}$ be a mode of $h(\theta)$ found via Newton-Raphson. We will need:

\begin{align*}
h_0 &= h(\wh{\theta}) = A \wh{\theta} - B(\wh{\theta} - C)^2 - D e^{\wh{\theta}} - Ee^{-\wh{\theta}} \\
h_1 &= h'(\wh{\theta}) = A  - 2 B(\wh{\theta} - C) - D e^{\wh{\theta}} + Ee^{-\wh{\theta}} \\
h_2 &= h''(\wh{\theta}) = - 2 B - D e^{\wh{\theta}} - Ee^{-\wh{\theta}} \\
\end{align*}

Now, we can approximate $h(\theta)$ with the quadratic Taylor approximation.

\begin{align*}
h(\theta) &\approx h_0 + h_1 (\theta - \wh{\theta}) + \frac{h_2}{2} (\theta - \wh{\theta})^2 
\end{align*}

$\wh{\theta}$ was found by setting the first derivative equal to 0, so $h_1$ must be 0. Hence,

\begin{align*}
h(\theta) &\approx h_0 + \frac{h_2}{2} (\theta - \wh{\theta})^2 \\
&= \left ( h_0 + \frac{h_2}{2} \wh{\theta}^2 \right )  - h_2 \wh{\theta} \theta + \frac{h_2}{2} \wh{\theta}^2 \\
&=  -\frac{1}{2 |h_2| \nv} \left (\theta - \wh{\theta}  \right )^2  + \text{ constant}
\end{align*}

Thus, one proposal distribution for $\theta$ in the Metropolis might be

\begin{align*}
N \left (\wh{\theta}, \ |h_2| \nv \right )
\end{align*}

whose density has the log kernel,

\begin{align*}
k(\theta) = -\frac{1}{2 |h_2| \nv} \left (\theta - \wh{\theta} \right )^2 
\end{align*}

However, this proposal is inefficient. To see this, consider the log ratio of the kernel of the proposal to the kernel of the full conditional.

\begin{align*}
\log \frac{\exp(k(\theta))}{p(\theta \mid \cdots)} &= \log q(\theta) - \log p(\theta \mid \cdots)  \\
&= -\frac{1}{2 |h_2| \nv} \left (\theta - \wh{\theta} \right )^2  - \left [  A \theta - B(\theta - C)^2 - D e^{\theta} - Ee^{-\theta} \right ] \\
&= -\frac{1}{2 |h_2| \nv} \left (\theta - \wh{\theta} \right )^2  -  A \theta + B(\theta - C)^2 + D e^{\theta} + Ee^{-\theta} \\
&= -\frac{1}{2 | - 2 B - D e^{\wh{\theta}} - Ee^{-\wh{\theta}}| \nv} \left (\theta - \wh{\theta} \right )^2  -  A \theta + B(\theta - C)^2 + D e^{\theta} + Ee^{-\theta} \\
&= - \left (B + \frac{D}{2} e^{\wh{\theta}} + \frac{E}{2}e^{-\wh{\theta}} \right) \left (\theta - \wh{\theta} \right )^2  -  A \theta + B(\theta - C)^2 + D e^{\theta} + Ee^{-\theta} \\
\end{align*} 

If $E > 0$, then

\begin{align*}
\lim_{\theta \rt -\infty} \log \frac{\exp(k(\theta))}{p(\theta \mid \cdots)} &= \lim_{\theta \rt -\infty} \left (- \left (B + \frac{D}{2} e^{\wh{\theta}}  \right) \left (\theta - \wh{\theta} \right )^2  -  A \theta + B(\theta - C)^2 + D e^{\theta} \right )\\
 &= \lim_{\theta \rt -\infty} \left (- \left (B + \frac{D}{2} e^{\wh{\theta}}  \right)  \theta^2 + B \theta^2 \right )\\
  &= \lim_{\theta \rt -\infty} \left (- \frac{D}{2} e^{\wh{\theta}}   \theta^2  \right )\\
  &= -\infty
\end{align*}

With a tail ratio of $-\infty$, this proposal is inefficient. A better proposal would be a Laplace($\wh{\theta}$, $s$) distribution, which has the log kernel,

\begin{align*}
q(\theta) &= - \frac{|\theta - \wh{\theta}|}{s}
\end{align*}

The variance of this distribution is $2 s^2$. The variance of the normal approximation was $|h_2|\nv$, so we can set those variances equal. Solving for $s$, we get $s = \frac{1}{2 \sqrt{|h_2|}}$. This proposal is safer because

\begin{align*}
\lim_{|\theta| \rt \infty} \log \frac{\exp(q(\theta))}{p(\theta \mid \cdots)} &= \lim_{|\theta| \rt \infty} \left ( - \frac{|\theta - \wh{\theta}|}{s}  -  A \theta + B(\theta - C)^2 + D e^{\theta} +  E e^{-\theta} \right ) \\
& \ge  \lim_{|\theta| \rt \infty} \left ( - \frac{|\theta - \wh{\theta}|}{s}  -  A \theta + B(\theta - C)^2 \right ) \\
&=  \lim_{|\theta| \rt \infty} \left ( - \frac{|\theta - \wh{\theta}|}{s}  -  A \theta + B(\theta - C)^2 \right ) \\
&= \infty \
\end{align*}

whether or not $E = 0$. \q

We easily can simulate from the Laplace distribution using the inverse cdf method. The cdf of the Laplace$(\wh{\theta}, \ s)$ distribution is

\begin{align*}
F(\theta) &= \begin{cases}
F_1(\theta) = \frac{1}{2} \exp \left ( \frac{\theta - \wh{\theta} }{s} \right ) & \theta < \wh{\theta}\\
F_2(\theta) = 1 - \frac{1}{2} \exp \left ( - \frac{\theta - \wh{\theta} }{s} \right ) & \theta \ge \wh{\theta}\\
\end{cases}
\end{align*}

Note that for $0 < u < \frac{1}{2}$,

\begin{align*}
u &= \frac{1}{2} \exp \left ( \frac{ F_1 \nv(u) - \wh{\theta} }{s} \right ) \\
\log (2u) &= \frac{ F_1 \nv(u) - \wh{\theta} }{s} \\
s \log (2 u) &= F_1 \nv(u) - \wh{\theta} \\
F_1 \nv(u) &=  s \log(2 u) + \wh{\theta}
\end{align*}

And for $\frac{1}{2} \le u < 1$,

\begin{align*}
u &= 1 -  \frac{1}{2} \exp \left (-  \frac{ F_2 \nv(u) - \wh{\theta} }{s} \right ) \\
2(1 - u) &=  \exp \left (-  \frac{ F_2 \nv(u) - \wh{\theta} }{s} \right ) \\
\log (2(1 - u)) &=  - \frac{ F_2 \nv(u) - \wh{\theta} }{s}  \\
-s \log (2(1 - u)) &=  F_2 \nv(u) - \wh{\theta}  \\
F_2 \nv(u) &= \wh{\theta} -s \log (2(1 - u))
\end{align*}

We can easily simulate from the Laplace distribution by taking $u \sim U(0, 1)$ and letting our simulated value, $\theta^*$, equal $F_1 \nv(u)$ if $0 < u < \frac{1}{2}$ and $F_2 \nv(u)$ if $\frac{1}{2} \le u < 1$/

\q
For the full sampler, let $\theta^{(i)}$ be the current value of $\theta$ at iteration $i$ of the algorithm. To get $\theta^{(i + 1)}$,

\begin{enumerate}
\item Sample a proposal $\theta^*$ from a $q(\theta)$.
\item The acceptance probability is

\begin{align*}
p &= \min \left \{1, \ \frac{\exp(h(\theta^*))}{\exp(h(\theta^{(i)}))} \frac{\exp(q(\theta^{(i)}))}{\exp(q(\theta^{*}))} \right \}
\end{align*}

In practice, calculate $p$ on a log scale.

\begin{align*}
\log p &= \min \left \{0, \ h(\theta^*) - h(\theta^{(i)}) +  q(\theta^{(i)}) - q(\theta^{*}) \right \}
\end{align*}

\item Sample $u \sim U(0, 1)$

\item If $\log u < \log p$, set $\theta^{(i + 1)} = \theta^{*}$ (accept $\theta^*$).. Otherwise, set $\theta^{(i+1)} = \theta^{(i)}$ (do not except $\theta^*$).

\end{enumerate}




















\section{The Full Metropolis-Within-Gibbs Sampler}

\paragraph{} By inspecting the full conditional distributions, one can see which parameters are conditionally independent. The plot below summarizes this conditional dependence.


\begin{center}
\includegraphics[scale=.4]{dependence}
\end{center}

Using this information, I can construct Gibbs steps within each of which the sampled parameters are conditionally independent.

\begin{enumerate}
\item $\rho_n$ ($n = 1, \ldots, N)$
\item $\gamma_g$ ($g = 1, \ldots, G)$
\item $\e_{g, n}$ ($g = 1, \ldots, G)$, ($n = 1, \ldots, N)$
\item $\phi_g$ ($g = 1, \ldots, G)$
\item $\alpha_g$ ($g = 1, \ldots, G)$
\item $\delta_g$ ($g = 1, \ldots, G)$
\item $\nu$, $\theta_\phi$, $\theta_\alpha$, $\theta_\delta$
\item $\tau$, $\sigma_\rho$, $\sigma_\phi$, $\sigma_\alpha$, $\sigma_\delta$ 
\end{enumerate}






\section{Diagnostics}


\subsection{Gelman Factors}

The potential scale reduction factor introduced in the textbook by Gelman \cite{gelman} monitors the lack of convergence of a single variable in an MCMC. Let $\eta_{ij}$ be the $i'th$ MCMC draw of a single variable in chain $j$. Then, the potential scale reduction factor, $\wh{R}$, compares the within-chain variance, $W$, to the between-chain variance, $B$. Suppose there are J chains, each with I iterations. Then, 

\begin{align*}
\wh{R} &= \sqrt{1 - \frac{1}{I} \left (\frac{B}{W} - 1 \right )} \\
B &= \frac{I}{J-1} \sum_{j = 1}^J (\ov{\eta}_{.j} - \ov{\eta}_{..})^2, \quad &&\ov{\eta}_{.j} = \frac{1}{I} \sum_{i = 1}^I \eta_{ij}, \quad \ov{\eta}_{..} \sum_{j = 1}^J \ov{\eta}_{.j} \\
W &= \frac{1}{J} \sum_{j = 1}^J s^2_j, && s_j^2 = \frac{1}{I - 1} \sum_{i = 1}^I (\eta_{ij} - \ov{\eta}_{.j})^2\\
\end{align*}

$\wh{R} \rt 1$ as $I \rt \infty$. An $\wh{R}$ value far above 1 indicates a lack of convergence, but an $\wh{R}$ value near 1 does not imply convergence. \q

The Gelman factor used in this analysis is not actually the one given above, but a degrees-of-freedom-adjusted version implemented in the {\tt gleman.diag()} function in the {\tt coda} package in R:

\begin{align*}
\wh{R} = \sqrt{\frac{d + 3}{d + 1} \frac{\wh{V}}{W}}
\end{align*}

where

\begin{align*}
d = 2 \frac{\wh{V}^2}{\text{Var}(\wh{V})}, \qquad \wh{V} &= \wh{\sigma}^2 + \frac{B}{IJ}, \qquad \wh{\sigma}^2 = \left (1 - \frac{1}{I} \right ) W + \frac{B}{I}
\end{align*}



\subsection{Deviance Information Criterion}

The deviance information criterion (DIC) is a model selection heuristic for hierarchical models much like the Akaike information criterion, AIC, and the Bayesian information criterion, BIC. As with AIC and BIC, given a set of models for $\vc{y}$, the one with the minimum DIC is preferred. DIC is based on the deviance, 

\begin{align*}
D(\vc{y}, \vc{\eta}) = -2 \log p(\vc{y} \mid \vc{\eta})
\end{align*}
where $\vc{y}$ is the data and $\vc{\eta}$ is the collection of model parameters. DIC itself is

\begin{align*}
\text{DIC} = 2 E(D(\vc{y}, \vc{\eta}) \mid \vc{y}) - D(\vc{y}, \wh{\vc{\eta}})
\end{align*}

where $\wh{\vc{\eta}}$ is a suitable point estimate of $\vc{\eta}$. If $\vc{\eta}_i$ is the collection of parameter estimates of iteration $i$ of the chain and $\ov{\vc{\eta}}$ is the collection of within-chain parameter means, then we can estimate DIC by

\begin{align*}
\wh{\text{DIC}} &=  \sum_{i = 1}^I [2 D(\vc{y} \mid \vc{\eta}_i)] - D(\vc{y}, \wh{\vc{\eta}}) \\
&= -4 \sum_{i = 1}^I \log p(\vc{y} \mid \vc{\eta}_i) + 2 \log p(\vc{y} \mid \ov{\vc{\eta}})
\end{align*}

All that remains is to find $\log p(\vc{y} \mid \vc{\eta})$ for a given set of parameters, $\vc{\eta}$. Let $\lambda_{g, n} = \exp(\rho_n + \e_{g, n} + \eta(g, n))$, where 

\begin{align*}
\eta(g, n) = \begin{cases}
\phi_g - \alpha_g & \text{library $n$ is in treatment group 1} \\
\phi_g + \delta_g & \text{library $n$ is in treatment group 2} \\
\phi_g + \alpha_g & \text{library $n$ is in treatment group 3}
\end{cases}
\end{align*}

\begin{align*}
\log p(\vc{y} \mid \vc{\eta}) &= \log \prod_{n = 1}^N \prod_{g = 1}^G \text{Poisson}( y_{g, n} \mid \lambda_{g, n}) \\
&= \sum_{n, g} \log \text{Poisson}( y_{g, n} \mid \lambda_{g, n}) \\
&= \sum_{n, g} \log \left (\frac{\exp(-\lambda_{g, n}) \lambda_{g, n}^{y_{g, n}}}{y_{g, n}!} \right) \\
&= \sum_{n, g}( -\lambda_{g, n} + y_{g, n} \log \lambda_{g, n} - \log (y_{g, n}!))
\end{align*}

Given the size of the data, calculating $ \sum_{n, g} - \log (y_{g, n}!)$ is intractable. Hence, in practice, we use

\begin{align*}
\text{DIC} = -4 \sum_{i = 1}^I L(\vc{y} \mid \vc{\eta}_i) + 2 L(\vc{y} \mid \ov{\vc{\eta}})
\end{align*}

where 

\begin{align*}
L(\vc{y}, \vc{\eta}) = \sum_{n, g}( -\lambda_{g, n} + y_{g, n} \log \lambda_{g, n}).
\end{align*}

This approach is reasonable because removing the $-\log (y_{g, n}!)$ term inside the sum merely offsets the DIC values of all the models under comparison by the same constant.
 
\appendix

\section{Derivations of the Full Conditionals}

Recall:

\begin{itemize}
\item  $k(n)$ = treatment group of library $n$.
\item $\lambda_{g, n} = \exp(\rho_n + \e_{g, n} + \eta(g, n))$ 
\item $G_\alpha = $ number of genes for which $\alpha_g \ne 0$
\item $G_\delta = $ number of genes for which $\delta_g \ne 0$
\end{itemize}

 Then from the model in Section \ref{sec:model}, we get: 

\begin{align*}
p(\nu \mid \cdots) &\propto \left [ \prod_{g = 1}^G \text{Inv-Gamma} \left ( \gamma_g^2 \ \left |  \ \text{shape} = \frac{\nu}{2} \right ., \text{scale} = \frac{\nu \tau^2}{2} \right ) \right ] \cdot \text{U}(\nu \mid 0, d) \\ 
p(\rho_n \mid \cdots) &\propto \left [ \prod_{g = 1}^G \text{Poisson}(y_{g, n} \mid \exp(\rho_n + \e_{g, n} + \eta(g, n))) \right ] \cdot  \text{N}(\rho_n \mid 0, \sigma_\rho^2) \\
p(\phi_g \mid \cdots) &\propto \left [ \prod_{n = 1}^N \text{Poisson}(y_{g, n} \mid \exp(\rho_n + \e_{g, n} + \eta(g, n))) \right ] \cdot \text{N}(\phi_g \mid \theta_\phi, \sigma_\phi^2) \\
p(\alpha_g \mid \cdots) &\propto \left [ \prod_{k(n) \ne 2} \text{Poisson}(y_{g, n} \mid \exp(\rho_n + \e_{g, n} + \eta(g, n))) \right ] \cdot \text{N}(\alpha_g \mid \theta_\alpha, \sigma_\alpha^2) \\
p(\delta_g \mid \cdots) &\propto \left [ \prod_{k(n) = 2} \text{Poisson}(y_{g, n} \mid \exp(\rho_n + \e_{g, n} + \eta(g, n))) \right ] \cdot \text{N}(\delta_g \mid \theta_\delta, \sigma_\delta^2) \\
p(\e_{g, n} \mid \cdots) &\propto \text{Poisson}(y_{g, n} \mid \exp(\rho_n + \e_{g, n} + \eta(g, n))) \cdot \text{N}(\e_{g, n} \mid 0, \gamma_g^2) \\
\end{align*}

\begin{align*}
p \left (\sigma_\rho \mid \cdots \right ) &= \left [ \prod_{n = 1}^N \text{N}(\rho_n \mid 0, \sigma_\rho^2) \right ] \cdot \text{U}(\sigma_\rho \mid 0, s_\rho) \\
p(\gamma_g^2 \mid \cdots) &\propto \left [ \prod_{n = 1}^N \text{N}(\e_{g, n} \mid 0, \gamma_g^2) \right ] \cdot \text{Inv-Gamma} \left ( \gamma_g^2 \ \left |  \ \text{shape} = \frac{\nu}{2} \right . , \text{scale} = \frac{\nu \tau^2}{2} \right ) \\ 
p(\tau^2 \mid \cdots) &\propto \left [ \prod_{g = 1}^G \text{Inv-Gamma} \left ( \gamma_g^2 \ \left |  \ \text{shape} = \frac{\nu}{2} \right ., \text{scale} = \frac{\nu \tau^2}{2} \right ) \right ] \cdot \text{Gamma}(\tau^2 \mid \text{shape} = a, \text{rate} = b) \\
p(\theta_\phi \mid \cdots ) & \propto \left [ \prod_{g = 1}^G \text{N}( \phi_g \mid \theta_\phi, \sigma_\phi^2) \right ]  \cdot \text{N}(\theta_\phi \mid 0, c_{\phi}^2) \\
p(\theta_\alpha \mid \cdots ) & \propto \left [ \prod_{g = 1}^G \text{N}( \alpha_g \mid \theta_\alpha, \sigma_\alpha^2) \right ]  \cdot \text{N}(\theta_\alpha \mid 0, c_{\alpha}^2) \\
p(\theta_\delta \mid \cdots ) & \propto \left [ \prod_{g = 1}^G \text{N}( \delta_g \mid \theta_\delta, \sigma_\delta^2) \right ]  \cdot \text{N}(\theta_\delta \mid 0, c_{\delta}^2) \\
p(\sigma_\phi \mid \cdots ) &\propto \left [ \prod_{g = 1}^G \text{N}( \phi_g \mid \theta_\phi, \sigma_\phi^2) \right ] \cdot \text{U}(\sigma_\phi \mid 0, s_{\phi }) \\
p(\sigma_\alpha \mid \cdots ) &\propto \left [ \prod_{g = 1}^G \text{N}( \alpha_g \mid \theta_\alpha, \sigma_\alpha^2) \right ] \cdot \text{U}(\sigma_\alpha \mid 0, s_{\alpha }) \\
p(\sigma_\delta \mid \cdots ) &\propto \left [ \prod_{g = 1}^G \text{N}( \delta_g \mid \theta_\delta, \sigma_\delta^2) \right ] \cdot \text{U}(\sigma_\delta \mid 0, s_{\delta }) \\
\end{align*}


\subsection{Transformations of Standard Deviations} \label{subsec:sd}

Let $\sigma$ be a standard deviation parameter and let $p(\sigma \mid \cdots)$ be its full conditional distribution. Then, by a transformation of variables, 

\begin{align*}
p(\sigma^2 \mid \cdots ) &= p(\sqrt{\sigma^2} \mid \cdots) \cdot \left | \frac{d}{d \sigma^2} \sqrt{\sigma^2}  \right | \\
&= p(\sigma \mid \cdots) \frac{1}{2} (\sigma^2)^{-1/2}
\end{align*}

I use this transformation several times in the next sections.


\subsection{$p(\nu \mid \cdots)$: Metropolis}

\begin{align*}
p(\nu \mid \cdots) &= \left [ \prod_{g = 1}^G \text{Inv-Gamma} \left ( \gamma_g^2 \mid \text{shape} = \frac{\nu}{2}, \ \text{scale} = \frac{\nu \tau^2}{2} \right ) \right ] \cdot \text{U}(\nu \mid 0, d) \\ 
&= \prod_{g = 1}^G \left [  \Gamma \left(\nu/2 \right )^{-1} \left ( \frac{\nu \tau^2}{2}\right ) ^ {\nu/2 } \left ( { \gamma_g^2} \right )^{ -(\nu/2 + 1)} \exp \left (- \frac{1}{ \gamma_g^2}\frac{\nu \tau^2}{2} \right ) \right ] I(0 < \nu < d) \\
&= \Gamma \left( \nu/2 \right )^{-G} \left ( \frac{\nu \tau^2}{2}\right ) ^ { G \nu  /2 } \left ( \prod_{g = 1}^G { \gamma_g^2} \right )^{ -(\nu/2 + 1)} \exp \left (- \frac{\nu \tau^2}{2} \sum_{g = 1}^G \frac{1}{ \gamma_g^2} \right ) I(0 < \nu < d) \\
& \propto \Gamma \left( \nu/2 \right )^{-G} \left ( \frac{\nu \tau^2}{2}\right ) ^ { G \nu  /2 } \left ( \prod_{g = 1}^G { \gamma_g^2} \right )^{ -\nu/2} \exp \left (- \frac{\nu \tau^2}{2} \sum_{g = 1}^G \frac{1}{ \gamma_g^2} \right ) I(0 < \nu < d) \\
&=  \exp \left ( -G \log \Gamma(\nu/2) + \frac{G \nu}{2} \log \left ( \frac{\nu \tau^2}{2} \right ) - \nu \sum_{g = 1}^G \log \gamma_g -\frac{\nu \tau^2}{2} \sum_{g = 1}^G \frac{1}{\gamma_g^2} \right ) \\
& \qquad \times I(0 < \nu < d) \\
&=  \exp \left ( -G \log \Gamma(\nu/2) + \frac{G \nu}{2} \log \left ( \frac{\nu \tau^2}{2} \right ) - \nu \sum_{g = 1}^G \left [ \log \gamma_g + \frac{\tau^2}{2} \frac{1}{\gamma_g^2} \right ] \right ) \\
& \qquad \times I(0 < \nu < d) \\
&=  \exp \left ( -G \log \Gamma(\nu/2) + \frac{G \nu}{2} \log \left ( \frac{\nu \tau^2}{2} \right ) - G \nu \frac{1}{G} \sum_{g = 1}^G \left [ \log \gamma_g + \frac{\tau^2}{2} \frac{1}{\gamma_g^2} \right ] \right ) \\
& \qquad \times I(0 < \nu < d) \\
&=  \exp \left ( - \log \Gamma(\nu/2) + \frac{ \nu}{2} \log \left ( \frac{\nu \tau^2}{2} \right ) -  \nu \frac{1}{G} \sum_{g = 1}^G \left [ \log \gamma_g + \frac{\tau^2}{2} \frac{1}{\gamma_g^2} \right ] \right )^G \\
& \qquad \times I(0 < \nu < d) \\
\end{align*}

\subsection{$p(\rho_n \mid \cdots)$: Metropolis}

\begin{align*}
p(\rho_n \mid \cdots) &\propto \left [ \prod_{g = 1}^G \text{Poisson}(y_{g, n} \mid \lambda_{g, n}) \right ] \cdot  \text{N}(\rho_n \mid 0, \sigma_\rho^2) \\
&\propto \left [ \prod_{g = 1}^G  \lambda_{g, n}^{y_{g, n}}  \exp(- \lambda_{g, n}) \right ] \exp \left ( - \frac{\rho_n^2}{2 \sigma_\rho^2} \right ) \\
&= \exp \left (\sum_{g = 1}^G  \left [ y_{g, n} \log \lambda_{g, n} - \lambda_{g, n} \right ] - \frac{\rho_n^2}{2 \sigma_\rho^2} \right ) \\
&=\exp \left (\sum_{g = 1}^G  \left [ y_{g, n} (\rho_n + \e_{g, n} + \eta(g, n)) - \exp(\rho_n + \e_{g, n} + \eta(g, n)) \right ] - \frac{\rho_n^2}{2 \sigma_\rho^2} \right ) \\
&=\exp \left (\rho_n G\ov{y}_{.n} +  \sum_{g = 1}^G \left [ y_{g, n}( \e_{g, n} + \eta(g, n)) \right ]  - \sum_{g = 1}^G \exp(\rho_n + \e_{g, n} + \eta(g, n))- \frac{\rho_n^2}{2 \sigma_\rho^2} \right ) \\
&\propto\exp \left (\rho_n G\ov{y}_{.n}  -\exp(\rho_n) \sum_{g = 1}^G \exp( \e_{g, n} + \eta(g, n))- \frac{\rho_n^2}{2 \sigma_\rho^2} \right ) \\
&\propto\exp \left (\rho_n G\ov{y}_{.n} - \frac{\rho_n^2}{2 \sigma_\rho^2} -\exp(\rho_n) \sum_{g = 1}^G \exp( \e_{g, n} + \eta(g, n)) \right ) 
\end{align*}






\subsection{ $p(\e_{g, n} \mid \cdots)$ Metropolis}
 
 \begin{align*}
p(\e_{g, n} \mid \cdots) &= \text{Poisson}(y_{g, n} \mid \lambda_{g, n}) \cdot \text{N}(\e_{g, n} \mid 0, \gamma_g^2) \\
&\propto \lambda_{g, n}^{y_{g, n}} \exp(- \lambda_{g,n}) \exp \left ( - \frac{\e_{g, n}^2}{2 \gamma_g^2} \right ) \\
&= \exp \left (y_{g, n} \log \lambda_{g, n}- \lambda_{g,n}  - \frac{\e_{g, n}^2}{2 \gamma_g^2} \right) \\
&= \exp \left (y_{g, n} (\rho_n + \e_{g, n} + \eta(g, n))- \exp(\rho_n + \e_{g, n} + \eta(g, n))  - \frac{\e_{g, n}^2}{2 \gamma_g^2} \right) \\
&= \exp \left (y_{g, n} \e_{g, n} - \exp(\rho_n + \e_{g, n} + \eta(g, n))  - \frac{\e_{g, n}^2}{2 \gamma_g^2} \right) \\
&= \exp \left (y_{g, n} \e_{g, n} - \frac{\e_{g, n}^2}{2 \gamma_g^2}  - \exp( \e_{g, n} ) \exp(\rho_n + \eta(g, n))  \right)
\end{align*}














\subsection{$p(\phi_g \mid \cdots)$: Metropolis}

\begin{align*}
p(\phi_g \mid \cdots) &= \left [ \prod_{n = 1}^N \text{Poisson}(y_{g, n} \mid \lambda_{g, n}) \right ] \cdot \text{N}(\phi_g \mid \theta_\phi, \sigma_\phi^2) \\
& \propto \left [ \prod_{n = 1}^N \lambda_{g, n}^{y_{g, n}} \exp(- \lambda_{g, n}) \right ] \cdot \exp \left ( - \frac{(\phi_g - \theta_\phi)^2}{2 \sigma_\phi^2} \right ) \\
&=  \exp \left (\sum_{n = 1}^N \left [y_{g, n} \log \lambda_{g, n}  - \lambda_{g, n} \right ] - \frac{(\phi_g - \theta_\phi)^2}{2 \sigma_\phi^2} \right ) \\
& = \exp \left (\sum_{n = 1}^N \left [y_{g, n} (\rho_n + \e_{g, n} + \eta(g, n))  - \exp (\rho_n + \e_{g, n} + \eta(g, n)) \right ] - \frac{(\phi_g - \theta_\phi)^2}{2 \sigma_\phi^2} \right ) \\
& \propto \exp \left (\sum_{n = 1}^N \left [y_{g, n} \eta(g, n)  - \exp (\rho_n + \e_{g, n} + \eta(g, n)) \right ] - \frac{(\phi_g - \theta_\phi)^2}{2 \sigma_\phi^2} \right ) \\
& \propto \exp \left (\sum_{n = 1}^N \left [y_{g, n} \eta(g, n) \right ]  - \sum_{n = 1}^N  \left [ \exp (\rho_n + \e_{g, n} + \eta(g, n)) \right ] - \frac{(\phi_g - \theta_\phi)^2}{2 \sigma_\phi^2} \right ) \\
& = \exp \left (\sum_{n = 1}^N \left [y_{g, n} \eta(g, n) \right ] - \frac{(\phi_g - \theta_\phi)^2}{2 \sigma_\phi^2}  - \sum_{n = 1}^N  \left [ \exp (\rho_n + \e_{g, n} + \eta(g, n)) \right ] \right ) \\
\end{align*}

and 

\begin{align*}
\sum_{n = 1}^N \left [y_{g, n} \eta(g, n) \right ]  &= \sum_{k(n) = 1} \left [y_{g, n} \eta(g, n) \right ]  +  \sum_{k(n) = 2} \left [y_{g, n} \eta(g, n) \right ]  +  \sum_{k(n) = 3} \left [y_{g, n} \eta(g, n) \right ] \\
 &= \sum_{k(n) = 1} \left [y_{g, n} (\phi_g - \alpha_g) \right ]  +  \sum_{k(n) = 2} \left [y_{g, n}(\phi_g + \delta_g) \right ]  +  \sum_{k(n) = 3} \left [y_{g, n} (\phi_g + \alpha_g) \right ] \\
&= \phi_g N \ov{y}_{g.} + \text{ constant}
\end{align*}

and 

\begin{align*}
\sum_{n = 1}^N  \left [ \exp (\rho_n + \e_{g, n} + \eta(g, n)) \right ]  &= \sum_{k(n) = 1}  \left [ \exp (\rho_n + \e_{g, n} + \eta(g, n)) \right ]  + \sum_{k(n) = 2}  \left [ \exp (\rho_n + \e_{g, n} + \eta(g, n)) \right ] \\
& \qquad + \sum_{k(n) = 3}  \left [ \exp (\rho_n + \e_{g, n} + \eta(g, n)) \right ] \\
&= \sum_{k(n) = 1}  \left [ \exp (\rho_n + \e_{g, n} + \phi_g - \alpha_g) \right ]  + \sum_{k(n) = 2}  \left [ \exp (\rho_n + \e_{g, n} + \phi_g + \delta_g) \right ] \\
& \qquad + \sum_{k(n) = 3}  \left [ \exp (\rho_n + \e_{g, n} + \phi_g + \alpha_g) \right ] \\
&= \exp(\phi_g) \left [ \sum_{k(n) = 1}  \left [ \exp (\rho_n + \e_{g, n} - \alpha_g) \right ]  + \sum_{k(n) = 2}  \left [ \exp (\rho_n + \e_{g, n} +  \delta_g) \right ]  \right . \\
& \qquad + \left .  \sum_{k(n) = 3}  \left [ \exp (\rho_n + \e_{g, n} +\alpha_g) \right ] \right ] \\
&= \exp(\phi_g) \left [\exp(-\alpha_g) \sum_{k(n) = 1}  \left [ \exp (\rho_n + \e_{g, n}) \right ]  + \exp(\delta_g) \sum_{k(n) = 2}  \left [ \exp (\rho_n + \e_{g, n} ) \right ]  \right . \\
& \qquad + \left . \exp(\alpha_g) \sum_{k(n) = 3}  \left [ \exp (\rho_n + \e_{g, n} ) \right ] \right ] \\
\end{align*}

so

\begin{align*}
p(\phi_g \mid \cdots) &\propto \exp \left ( \phi_g N \ov{y}_{g.}  - \frac{(\phi_g - \theta_\phi)^2}{2 \sigma_\phi^2} - \exp(\phi_g) \left [\exp(-\alpha_g) \sum_{k(n) = 1}  \left [ \exp (\rho_n + \e_{g, n}) \right ]  \right . \right . \\
& \left . \qquad \exp(\delta_g) \sum_{k(n) = 2}  \left [ \exp (\rho_n + \e_{g, n} ) \right ]   + \left . \exp(\alpha_g) \sum_{k(n) = 3}  \left [ \exp (\rho_n + \e_{g, n} ) \right ] \right ] \right )
\end{align*}



\subsection{$p (\alpha_g \mid \cdots) $: Metropolis}
Similar to $\phi_g$,

\begin{align*}
p(\alpha_g \mid \cdots ) & \propto \exp \left (\sum_{k(n) \ne 2} \left [y_{g, n} \eta(g, n) \right ] - \frac{(\alpha_g - \theta_\alpha)^2}{2 \sigma_\alpha^2}  - \sum_{k(n) \ne 2}  \left [ \exp (\rho_n + \e_{g, n} + \eta(g, n)) \right ] \right ) \\
\end{align*}

and

\begin{align*}
\sum_{k(n) \ne 2} \left [y_{g, n} \eta(g, n) \right ]  &= \sum_{k(n) = 1} \left [y_{g, n} \eta(g, n) \right ]  +  \sum_{k(n) = 3} \left [y_{g, n} \eta(g, n) \right ] \\
 &= \sum_{k(n) = 1} \left [y_{g, n} (\phi_g - \alpha_g) \right ]   +  \sum_{k(n) = 3} \left [y_{g, n} (\phi_g + \alpha_g) \right ] \\
&= \alpha_g \left (\sum_{k(n) = 3} y_{g, n} - \sum_{k(n) = 1} y_{g, n} \right ) + \text{ constant}
\end{align*}

and


\begin{align*}
\sum_{k(n) \ne 2}  \exp (\rho_n &+  \e_{g, n} + \eta(g, n)) = \sum_{k(n) = 1}  \left [ \exp (\rho_n + \e_{g, n} + \eta(g, n)) \right ]  + \sum_{k(n) = 3}  \left [ \exp (\rho_n + \e_{g, n} + \eta(g, n)) \right ] \\
&= \sum_{k(n) = 1}  \left [ \exp (\rho_n + \e_{g, n} + \phi_g - \alpha_g) \right ]  + \sum_{k(n) = 3}  \left [ \exp (\rho_n + \e_{g, n} + \phi_g + \alpha_g) \right ] \\
&= \exp(-\alpha_g) \exp(\phi_g) \sum_{k(n) = 1}  \left [ \exp (\rho_n + \e_{g, n}) \right ]  + \exp(\alpha_g) \exp(\phi_g) \sum_{k(n) = 3}  \left [ \exp (\rho_n + \e_{g, n}) \right ] \\
\end{align*}

so

\begin{align*}
p(\alpha_g \mid \cdots) &\propto \exp \left (\alpha_g \left (\sum_{k(n) = 3} y_{g, n} - \sum_{k(n) = 1} y_{g, n} \right ) - \frac{(\alpha_g - \theta_\alpha)^2}{2 \sigma_\alpha^2} - \exp(-\alpha_g) \exp(\phi_g) \sum_{k(n) = 1}  \left [ \exp (\rho_n + \e_{g, n}) \right ]  \right . \\
& \qquad \left .- \exp(\alpha_g) \exp(\phi_g) \sum_{k(n) = 3}  \left [ \exp (\rho_n + \e_{g, n}) \right ] \right )
\end{align*}


\subsection{$p (\delta_g \mid \cdots)$: Metropolis}


Similar to $\phi_g$,

\begin{align*}
p(\delta_g \mid \cdots ) & \propto \exp \left (\sum_{k(n) = 2} \left [y_{g, n} \eta(g, n) \right ] - \frac{(\delta_g - \theta_\delta)^2}{2 \sigma_\delta^2}  - \sum_{k(n) \ne 2}  \left [ \exp (\rho_n + \e_{g, n} + \eta(g, n)) \right ] \right ) \\
\end{align*}

and 

\begin{align*}
\sum_{k(n) = 2} \left [y_{g, n} \eta(g, n) \right ] &= \sum_{k(n) = 2} \left [y_{g, n} (\phi_g + \delta_g) \right ] \\
&= \delta_g \sum_{k(n) = 2} y_{g, n} + \text{ constant}
\end{align*}

and

\begin{align*}
\sum_{k(n) \ne 2} \exp (\rho_n + \e_{g, n} + \eta(g, n)) &=  \sum_{k(n) \ne 2} \exp (\rho_n + \e_{g, n} + \phi_g + \delta_g) \\
&= \exp(\delta_g) \exp(\phi_g) \sum_{k(n) \ne 2} \exp (\rho_n + \e_{g, n}) 
\end{align*}

so



\begin{align*}
p(\delta_g \mid \cdots ) & \propto \exp \left (\delta_g \sum_{k(n) = 2} y_{g, n}  - \frac{(\delta_g - \theta_\delta)^2}{2 \sigma_\delta^2}  - \exp(\delta_g) \exp(\phi_g) \sum_{k(n) \ne 2} \exp (\rho_n + \e_{g, n})  \right ) \\
\end{align*}










%%%%%%%%%%%%%%%%%%%%%%%%%





\subsection{$p(\theta_\phi \mid \cdots )$: Normal}


\begin{align*}
p(\theta_\phi \mid \cdots ) & = \left [ \prod_{g = 1}^G \text{N}( \phi_g \mid \theta_\phi, \sigma_\phi^2) \right] \cdot \text{N}(\theta_\phi \mid 0, c_\phi^2) \\
&\propto \left [ \prod_{g = 1}^G \exp \left ( -\frac{(\phi_g - \theta_\phi)^2}{2 \sigma_\phi^2} \right) \right ] \exp \left ( - \frac{\theta_\phi^2}{2 c_\phi^2} \right ) \\
&=  \exp \left ( - \sum_{g = 1}^G \frac{(\phi_g - \theta_\phi)^2}{2 \sigma_\phi^2} \right)  \exp \left ( - \frac{\theta_\phi^2}{2 c_\phi^2} \right ) \\
&=  \exp \left ( -  \frac{\sum_{g = 1}^G\phi_g^2 -2  \theta_\phi \sum_{g = 1}^G \phi_g + G \theta_\phi^2}{2 \sigma_\phi^2} \right)  \exp \left ( - \frac{\theta_\phi^2}{2 c_\phi^2} \right ) \\
&= \exp \left ( -  \frac{\sum_{g = 1}^G\phi_g^2 -2  \theta_\phi \sum_{g = 1}^G \phi_g + G \theta_\phi^2}{2 \sigma_\phi^2}  - \frac{\theta_\phi^2}{2 c_\phi^2} \right ) \\
&= \exp \left ( -  \frac{ c_\phi^2 \sum_{g = 1}^G \phi_g^2 -2  c_\phi^2 (\sum_{g = 1}^G \phi_g) \theta_\phi + G c_\phi^2 \theta_\phi^2}{2 \sigma_\phi^2 c_\phi^2}  - \frac{\sigma_\phi^2 \theta_\phi^2}{2 \sigma_\phi^2 c_\phi^2} \right ) \\
&= \exp \left ( -  \frac{ c_\phi^2 \sum_{g = 1}^G \phi_g^2 -2  c_\phi^2 (\sum_{g = 1}^G \phi_g) \theta_\phi + (G c_\phi^2 + \sigma_\phi^2)\theta_\phi^2}{2 \sigma_\phi^2 c_\phi^2} \right ) \\
&\propto \exp \left ( -  \frac{ (G c_\phi^2 + \sigma_\phi^2) \left (\theta_\phi - \frac{c_\phi^2  (\sum_{g = 1}^G \phi_g)}{G c_\phi^2 + \sigma_\phi^2} \right )^2}{2 \sigma_\phi^2 c_\phi^2} \right ) 
\end{align*}

Hence:

\begin{align*}
p(\theta_\phi \mid \cdots) &= \text{N} \left ( \theta_\phi \ \left | \ \frac{ c_\phi^2 \sum_{g = 1}^G \phi_g}{G c_\phi^2 + \sigma_\phi^2}, \ \frac{c_\phi^2 \sigma_\phi^2}{Gc_\phi^2 + \sigma_\phi^2} \right . \right )
\end{align*}

\subsection{$p \left (\theta_\alpha \mid \cdots \right )$: Normal}

\begin{align*}
p(\theta_\alpha \mid \cdots )& \propto \left [ \prod_{g = 1}^G \text{N}(\alpha_g \mid \theta_\alpha, \sigma_\alpha^2)]  \right ]  \cdot \text{N}(\theta_\alpha \mid 0, c_\alpha^2)
\end{align*}

From algebra similar to the derivation of $p(\theta_\phi \mid \cdots)$,

\begin{align*}
p(\theta_\alpha \mid \cdots) &= N \left ( \theta_\alpha \ \left | \ \frac{c_\alpha^2 \sum_{g = 1}^G \alpha_g}{G_\alpha c_\alpha^2 + \sigma_\alpha^2}, \right . \ \frac{c_\alpha^2 \sigma_\alpha^2}{G_\alpha c_\alpha^2 + \sigma_\alpha^2} \right ) 
\end{align*}

\subsection{$p(\theta_\delta \mid \cdots )$: Normal}

\begin{align*}
p(\theta_\delta \mid \cdots )& \propto \left [ \prod_{g = 1}^G \text{N}(\delta_g \mid \theta_\delta, \sigma_\delta^2)]  \right ]  \cdot \text{N}(\theta_\delta \mid 0, c_\delta^2)
\end{align*}

From algebra similar to the derivation of $p(\theta_\phi \mid \cdots)$,

\begin{align*}
p(\theta_\delta \mid \cdots) &= N \left ( \theta_\delta \ \left | \ \frac{c_\delta^2 \sum_{g = 1}^G \delta_g}{G_\delta c_\delta^2 + \sigma_\delta^2}, \right . \ \frac{c_\delta^2 \sigma_\delta^2}{G_\delta c_\delta^2 + \sigma_\delta^2} \right ) 
\end{align*}



%%%%%%%%%%%%%%%%%%

\subsection{$p(\tau^2 \mid \cdots)$: Gamma}

\begin{align*}
p(\tau^2 \mid \cdots) &= \left [ \prod_{g = 1}^G \text{Inv-Gamma} \left ( \gamma_g^2 \mid \text{shape} = \frac{\nu}{2}, \text{scale} = \frac{\nu \tau^2}{2} \right ) \right ] \cdot \text{Gamma}(\tau^2 \mid \text{shape} = a, \text{rate} = b) \\
&\propto \left [ \Gamma \left(\nu/2 \right )^{-G} \left ( \frac{\nu \tau^2}{2}\right ) ^ { G \nu  /2 } \left ( \prod_{g = 1}^G { \gamma_g^2} \right )^{ -(\nu/2 + 1)} \exp \left (- \frac{\nu \tau^2}{2} \sum_{g = 1}^G \frac{1}{ \gamma_g^2} \right ) \right ] \cdot (\tau^2)^{a - 1} \exp \left (- b \tau^2 \right )  \\
& \propto \left [ \left ( \tau^2 \right ) ^ { G \nu  /2 } \exp \left (- \tau^2 \cdot \frac{\nu}{2} \sum_{g = 1}^G \frac{1}{ \gamma_g^2} \right ) \right ] \cdot (\tau^2)^{a - 1} \exp \left (- b \tau^2 \right )  \\
&= (\tau^2)^{G\nu/2 + a - 1} \exp \left (- \tau^2 \left (b + \frac{\nu}{2} \sum_{g = 1}^G \frac{1}{\gamma_g^2} \right )  \right ) 
\end{align*}

Hence:

\begin{align*}
p(\tau^2 \mid \cdots) = \text{Gamma} \left ( \tau^2 \mid \text{shape} =  a + \frac{G\nu}{2}, \ \text{rate} =  b + \frac{\nu}{2} \sum_{g = 1}^G \frac{1}{\gamma_g^2} \right ) 
\end{align*}


\subsection{$p \left (\frac{1}{\gamma_g^2} \mid \cdots \right )$ Gamma}

\begin{align*}
p(\gamma_g^2 \mid \cdots) &= \left [ \prod_{n = 1}^N \text{N}(\e_{g, n} \mid 0, \gamma_g^2) \right ] \cdot \text{Inv-Gamma} \left ( \gamma_g^2 \mid \text{shape} = \frac{\nu}{2}, \text{scale} = \frac{\nu \tau^2}{2} \right ) \\ 
&\propto \left [ \prod_{n = 1}^N (\gamma_g^{2})^{-1/2} \exp \left (- \frac{1}{ \gamma_g^2} \frac{\e_{g, n}^2}{2} \right ) \right ] \cdot \left ( { \gamma_g^2} \right )^{ -(\nu/2 + 1)} \exp \left (- \frac{1}{ \gamma_g^2}\frac{\nu \tau^2}{2} \right ) \\
&=  \left [ (\gamma_g^{2})^{-N/2} \exp \left (-\frac{1}{ \gamma_g^2}  \frac{1}{2} \sum_{n = 1}^N \e_{g, n}^2 \right ) \right ] \cdot \left ( { \gamma_g^2} \right )^{ -(\nu/2 + 1)} \exp \left (- \frac{1}{ \gamma_g^2}\frac{\nu \tau^2}{2} \right ) \\
&=  (\gamma_g^{2})^{-((N+ \nu)/2 + 1)} \exp \left (-\frac{1}{ \gamma_g^2}  \frac{1}{2} \left (\nu \tau^2 + \sum_{n = 1}^N \e_{g, n}^2 \right ) \right ) 
\end{align*}

which is the kernel of an inverse gamma distribution. Hence:

\begin{align*}
p \left ( \left . \frac{1}{\gamma_g^2} \ \right | \ \cdots \right ) = \text{Gamma} \left ( \left . \frac{1}{\gamma_g^2} \ \right | \ \text{shape} = \frac{N + \nu}{2}, \  \text{rate} = \frac{1}{2} \left (\nu \tau^2 + \sum_{n  =1}^N \e_{g, n}^2 \right ) \right )
\end{align*}



\subsection {$p \left ( \frac{1}{\sigma_\rho^2} \mid \cdots \right ) $ Truncated Gamma}

\begin{align*}
p( \sigma_\rho^2 \mid \cdots) &= p(\sigma_\rho \mid \cdots) \frac{1}{2} (\sigma_\rho^2)^{-1/2} \qquad \text{(transformation in Section \ref{subsec:sd})} \\
 &\propto \left [ \prod_{n = 1}^N \text{N}(\rho_n \mid 0, \sigma_\rho^2) \right ] \cdot \text{U}(\sigma_\rho \mid 0, s_\rho) \frac{1}{2} (\sigma_\rho^2)^{-1/2} \\
    &\propto \prod_{n = 1}^N \left [ \frac{1}{\sqrt{\sigma_\rho^2 }}\exp \left (- \frac{\rho_n^2}{2 \sigma_\rho^2} \right ) \right ] \cdot \text{I}(0 < \sigma_\rho < s_\rho) (\sigma_\rho^2)^{-1/2}\\
    &= (\sigma_\rho^2)^{-N/2} \exp \left ( - \frac{1}{\sigma_\rho^2}\frac{1}{2 } \sum_{n = 1}^N \rho_n^2 \right )\cdot \text{I}(0 < \sigma_\rho < s_\rho) (\sigma_\rho^2)^{-1/2} \\
&= (\sigma_\rho^2)^{-(N/2 -1/2 + 1)} \exp \left ( - \frac{1}{\sigma_\rho^2}\frac{1}{2 } \sum_{n = 1}^N \rho_n^2 \right )\cdot \text{I}(0 < \sigma_\rho < s_\rho) \\
 \end{align*}
 
which is the kernel of a truncated inverse gamma distribution. Hence:

\begin{align*}
p \left ( \frac{1}{\sigma_\rho^2} \mid \cdots \right) &= \text{Gamma} \left ( \frac{1}{\sigma_\rho^2} \mid \text{shape} = \frac{N - 1}{2}, \ \text{rate} =\frac{1}{2} {\sum_{n = 1}^N \rho_n^2} \right )  I \left (\frac{1}{\sigma_\rho^2} > \frac{1}{s_\rho^2} \right ) \\
\end{align*}





\subsection{$p\left ( \frac{1}{\sigma_\phi^2} \mid \ldots \right )$: Truncated Gamma}

\begin{align*}
p(\sigma_\phi^2 \mid \cdots ) &= p(\sigma_\phi \mid \cdots) \frac{1}{2} (\sigma_\phi^2)^{-1/2} \qquad \text{(transformation in Section \ref{subsec:sd})} \\
&\propto \left [ \prod_{g = 1}^G \text{N}( \phi_g \mid \theta_\phi, \sigma_\phi^2) \right ] \cdot \text{U}(\sigma_\phi \mid 0, s_\phi)  (\sigma_\phi^2)^{-1/2}  \\ 
&\propto \left [ \prod_{g = 1}^G (\sigma_\phi^2)^{-1/2} \exp \left ( - \frac{(\phi_g - \theta_\phi)^2}{2 \sigma_\phi^2} \right )
 \right ] \text{I}(0 < \sigma_\phi^2 < s_\phi^2) (\sigma_\phi^2)^{-1/2} \\
&=  (\sigma_\phi^2)^{-G/2} \exp \left ( - \sum_{g = 1}^G \frac{(\phi_g - \theta_\phi)^2}{2 \sigma_\phi^2} \right ) \text{I}(0 < \sigma_\phi^2 < s_\phi^2) (\sigma_\phi^2)^{-1/2} \\
&=  (\sigma_\phi^2)^{-(G/2 - 1/2 +1) } \exp \left ( - \frac{1}{\sigma_\phi^2} \frac{1}{2} \sum_{g = 1}^G (\phi_g - \theta_\phi)^2 \right ) \text{I}(0 < \sigma_\phi^2 < s_\phi^2) \\
\end{align*}

which is the kernel of a truncated inverse gamma distribution. Hence:

\begin{align*}
p \left ( \left . \frac{1}{\sigma_\phi^2} \  \right | \  \cdots \right ) &= \text{Gamma} \left ( \text{shape} = \frac{G - 1}{2}, \ \text{rate} =  \frac{1}{2} \sum_{g = 1}^G (\phi_g - \theta_\phi)^2  \right )   \text{I} \left (\frac{1}{\sigma_\phi^2} >\frac{1}{ s_\phi^2} \right )
\end{align*}

\subsection{$p \left (\frac{1}{\sigma_\alpha^2} \mid \cdots \right )$: Truncated Gamma}

Analogously to $\sigma_\phi$,

\begin{align*}
p \left ( \left . \frac{1}{\sigma_\alpha^2} \  \right | \  \cdots \right ) &= \text{Gamma} \left ( \text{shape} = \frac{G - 1}{2}, \ \text{rate} =  \frac{1}{2} \sum_{g = 1}^G (\alpha_g - \theta_\alpha)^2  \right )   \text{I} \left (\frac{1}{\sigma_\alpha^2} >\frac{1}{ s_\alpha^2} \right )
\end{align*}


\subsection{$p \left (\frac{1}{\sigma_\delta^2} \mid \cdots  \right )$: Truncated Gamma}

Analogously to $\sigma_\phi$,

\begin{align*}
p \left ( \left . \frac{1}{\sigma_\delta^2} \  \right | \  \cdots \right ) &= \text{Gamma} \left ( \text{shape} = \frac{G - 1}{2}, \ \text{rate} =  \frac{1}{2} \sum_{g = 1}^G (\delta_g - \theta_\delta)^2  \right )   \text{I} \left (\frac{1}{\sigma_\delta^2} >\frac{1}{ s_\delta^2} \right )
\end{align*}





\end{flushleft}
%\newpage 
%\bibliographystyle{plainnat} 
%\bibliography{method}
\end{document}